{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGPRiqPYShUV"
      },
      "source": [
        "Training a custom dataset with YOLOv5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiG--izm3-Jk"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCK-8W99Sg03",
        "outputId": "fbbe855e-aebe-4e14-ed91-e3a9446fca68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FallDetectionPipeline'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 38 (delta 12), reused 22 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), 13.02 MiB | 9.53 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Billy-06/FallDetectionPipeline.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaJRR2PKo1RP",
        "outputId": "29a509e3-da0e-41d5-d6bd-16068122f965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 23 23:20:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx4mVjKYTkai",
        "outputId": "cf3e3360-f969-4710-964c-6d4d3f263640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.0.1+cu118 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15101MB, multi_processor_count=40)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from IPython.display import Image  # for displaying images\n",
        "# from utils.google_utils import gdrive_download  # for downloading models/datasets \n",
        "\n",
        "print('Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6rAiX6XZ3vO",
        "outputId": "a1440989-9ca2-4504-e4d6-37ef6fe5c9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VTlT3Y7e1h-Zsne4zahjx5q0TK2ClMVv\n",
            "To: /content/LLVIP.zip\n",
            "100% 4.00G/4.00G [00:37<00:00, 108MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1VTlT3Y7e1h-Zsne4zahjx5q0TK2ClMVv\n",
        "# https://drive.google.com/file/d/1VTlT3Y7e1h-Zsne4zahjx5q0TK2ClMVv/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BjurgH--Xc",
        "outputId": "93370f9a-4cff-4d3c-8818-0ae589c684c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FallDetectionPipeline  LLVIP.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA4lBfEM_VX9"
      },
      "outputs": [],
      "source": [
        "!unzip LLVIP.zip -d FallDetectionPipeline/my_data/\n",
        "!rm -f LLVIP.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od2l5Q3SAZHy",
        "outputId": "12758ea7-cf55-4f43-b07f-89a3f6121a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FallDetectionPipeline\n"
          ]
        }
      ],
      "source": [
        "%cd FallDetectionPipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd my_data/LLVIP/visible/\n",
        "!mkdir images\n",
        "!mkdir labels\n",
        "\n",
        "%cd ../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3niYbcJkUPx",
        "outputId": "e0ed63e5-f95c-461a-feb4-a6cb31bca950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FallDetectionPipeline/my_data/LLVIP/visible\n",
            "/content/FallDetectionPipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTM38_rqA5sm",
        "outputId": "c4627e82-8887-4578-b18a-5a009de8bacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes        __init__.py\tmy_data\t\t  yolotest.ipynb\n",
            "classes.txt    llvip_data.yaml\trequirements.txt  yolov5\n",
            "copyImages.py  main.py\t\txmlToYolo.py\t  yolov5_cv.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kdMqIl1bOYB",
        "outputId": "04e128eb-781f-4e9a-c24c-3d8032eb4252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15705, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 15705 (delta 9), reused 23 (delta 6), pack-reused 15672\u001b[K\n",
            "Receiving objects: 100% (15705/15705), 14.50 MiB | 16.50 MiB/s, done.\n",
            "Resolving deltas: 100% (10754/10754), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYv7FAggPf4a",
        "outputId": "0207b690-822e-4b08-be67-0fb3febb0719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied 3000 files to 'my_data/LLVIP/visible/val/'\n"
          ]
        }
      ],
      "source": [
        "%%python3 copyImages.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq8MhyQmBF0J"
      },
      "outputs": [],
      "source": [
        "%%python3 xmlToYolo.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd my_data/LLVIP/visible/\n",
        "!mv train images\n",
        "!mv val images\n",
        "\n",
        "%cd ../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F72OPA_k8bi",
        "outputId": "46d39d59-a033-4364-cf5c-8f352ad7e97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FallDetectionPipeline/my_data/LLVIP/visible\n",
            "/content/FallDetectionPipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKCliw7HCyOh",
        "outputId": "034e3fed-e74d-4c19-93e1-b38fd1d3a655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes        __init__.py\tmy_data\t\t  yolotest.ipynb\n",
            "classes.txt    llvip_data.yaml\trequirements.txt  yolov5\n",
            "copyImages.py  main.py\t\txmlToYolo.py\t  yolov5_cv.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg8WRPUXDXXp",
        "outputId": "3fe65e63-56cf-478f-eecd-2a472ea73ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FallDetectionPipeline/yolov5\n"
          ]
        }
      ],
      "source": [
        "%cd yolov5/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "659ErzpHQgAv",
        "outputId": "d31be53c-2813-4145-c4cf-001ce46d7ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "benchmarks.py\t data\t     LICENSE\t      requirements.txt\ttutorial.ipynb\n",
            "CITATION.cff\t detect.py   models\t      segment\t\tutils\n",
            "classify\t export.py   README.md\t      setup.cfg\t\tval.py\n",
            "CONTRIBUTING.md  hubconf.py  README.zh-CN.md  train.py\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swgWi545SdPE"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdHdNFE78_Qx"
      },
      "outputs": [],
      "source": [
        "# !pip install accelerate\n",
        "# import torch\n",
        "# from torchvision import transforms\n",
        "\n",
        "# p = transforms.Compose([transforms.Scale((48,48))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3jZuXnzSglT",
        "outputId": "ca3b4f4f-1fb6-4914-c563-09631f789350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=llvip_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=4, batch_size=24, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0, 1, 2, 3, 4, 5], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-169-geef637c Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 39.6MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 174MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "freezing model.0.conv.weight\n",
            "freezing model.0.bn.weight\n",
            "freezing model.0.bn.bias\n",
            "freezing model.1.conv.weight\n",
            "freezing model.1.bn.weight\n",
            "freezing model.1.bn.bias\n",
            "freezing model.2.cv1.conv.weight\n",
            "freezing model.2.cv1.bn.weight\n",
            "freezing model.2.cv1.bn.bias\n",
            "freezing model.2.cv2.conv.weight\n",
            "freezing model.2.cv2.bn.weight\n",
            "freezing model.2.cv2.bn.bias\n",
            "freezing model.2.cv3.conv.weight\n",
            "freezing model.2.cv3.bn.weight\n",
            "freezing model.2.cv3.bn.bias\n",
            "freezing model.2.m.0.cv1.conv.weight\n",
            "freezing model.2.m.0.cv1.bn.weight\n",
            "freezing model.2.m.0.cv1.bn.bias\n",
            "freezing model.2.m.0.cv2.conv.weight\n",
            "freezing model.2.m.0.cv2.bn.weight\n",
            "freezing model.2.m.0.cv2.bn.bias\n",
            "freezing model.3.conv.weight\n",
            "freezing model.3.bn.weight\n",
            "freezing model.3.bn.bias\n",
            "freezing model.4.cv1.conv.weight\n",
            "freezing model.4.cv1.bn.weight\n",
            "freezing model.4.cv1.bn.bias\n",
            "freezing model.4.cv2.conv.weight\n",
            "freezing model.4.cv2.bn.weight\n",
            "freezing model.4.cv2.bn.bias\n",
            "freezing model.4.cv3.conv.weight\n",
            "freezing model.4.cv3.bn.weight\n",
            "freezing model.4.cv3.bn.bias\n",
            "freezing model.4.m.0.cv1.conv.weight\n",
            "freezing model.4.m.0.cv1.bn.weight\n",
            "freezing model.4.m.0.cv1.bn.bias\n",
            "freezing model.4.m.0.cv2.conv.weight\n",
            "freezing model.4.m.0.cv2.bn.weight\n",
            "freezing model.4.m.0.cv2.bn.bias\n",
            "freezing model.4.m.1.cv1.conv.weight\n",
            "freezing model.4.m.1.cv1.bn.weight\n",
            "freezing model.4.m.1.cv1.bn.bias\n",
            "freezing model.4.m.1.cv2.conv.weight\n",
            "freezing model.4.m.1.cv2.bn.weight\n",
            "freezing model.4.m.1.cv2.bn.bias\n",
            "freezing model.5.conv.weight\n",
            "freezing model.5.bn.weight\n",
            "freezing model.5.bn.bias\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005625000000000001), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/FallDetectionPipeline/my_data/LLVIP/visible/labels/train... 12023 images, 2 backgrounds, 0 corrupt: 100% 12025/12025 [00:06<00:00, 1834.22it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/FallDetectionPipeline/my_data/LLVIP/visible/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/FallDetectionPipeline/my_data/LLVIP/visible/labels/val... 3000 images, 0 backgrounds, 0 corrupt: 100% 3000/3000 [00:03<00:00, 982.32it/s] \n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/FallDetectionPipeline/my_data/LLVIP/visible/labels/val.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.75 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 4 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/3      6.15G    0.06156    0.04067   0.004353          0       1024: 100% 502/502 [24:38<00:00,  2.94s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 63/63 [02:56<00:00,  2.80s/it]\n",
            "                   all       3000       8545       0.86      0.736      0.816      0.364\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/3      8.61G    0.04787    0.03249  0.0006453        118       1024:  69% 346/502 [17:19<08:03,  3.10s/it]"
          ]
        }
      ],
      "source": [
        "!python train.py --img 1024 --batch-size 24 --epochs 4 --data llvip_data.yaml --freeze 0 1 2 3 4 5 --weights yolov5s.pt --workers=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIoA2_zdS9hF"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 9025 --batch 20 --epochs 30 --data llvip_data.yaml --weights yolov5s.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLaVlhHyxNe1"
      },
      "source": [
        "/content/FallDetectionPipeline/my_data/LLVIP/visible/images/train"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}